"""
å®Œæ•´æ”¹è¿›ç‰ˆRAGç³»ç»Ÿ
ä¿ç•™ä½ æ‰€æœ‰åŸæœ‰åŠŸèƒ½ï¼Œæ–°å¢ï¼š
1. ç›¸ä¼¼åº¦é˜ˆå€¼æ£€æŸ¥
2. é—®é¢˜æ”¹å†™é‡è¯•
3. é˜²æ­¢LLMèƒ¡ç¼–ä¹±é€ 
"""

import pandas as pd
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
import os
import re
from pathlib import Path
from config import Config
Config.setup_environment()


class EnhancedRAGSystem:
    """æ”¹è¿›ç‰ˆRAGç³»ç»Ÿ - åœ¨åŸæœ‰åŸºç¡€ä¸Šæ–°å¢ç›¸ä¼¼åº¦æ£€æŸ¥"""
    
    # æ–°å¢é…ç½®
    SIMILARITY_THRESHOLD = 0.3  # ç›¸ä¼¼åº¦é˜ˆå€¼
    MAX_REWRITE_ATTEMPTS = 1    # æœ€å¤§æ”¹å†™æ¬¡æ•°
    
    def __init__(self, 
                 data_csv: str = Config.BATCH_CLASSIFICATION,
                 insights_csv: str = "results/analysis_insights.csv",
                 index_path: str = Config.FAISS_INDEX_PATH):
        """
        åˆå§‹åŒ–RAGç³»ç»Ÿ
        
        Args:
            data_csv: åŸå§‹åœ°åæ•°æ®CSV
            insights_csv: åˆ†ææ´å¯ŸCSV
            index_path: å‘é‡åº“å­˜å‚¨è·¯å¾„
        """
        self.data_csv = data_csv
        self.insights_csv = insights_csv
        self.index_path = index_path
        
        # APIé…ç½®
        os.environ["OPENAI_API_KEY"] = "sk-gswitcfpsevlgfleazpwptqtpuolngnbzqvtbkeuexeqiyid"
        os.environ["OPENAI_BASE_URL"] = "https://api.siliconflow.cn/v1"
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.embeddings = None
        self.vectorstore = None
        self.llm = None
        self.rag_chain = None
    
    def setup(self):
        """è®¾ç½®RAGç³»ç»Ÿ"""
        print("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–å¢å¼ºç‰ˆRAGç³»ç»Ÿ...")
        
        # 1. åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        self.embeddings = OpenAIEmbeddings(
            model=Config.EMBEDDING_MODEL,
            chunk_size=64
        )
        
        # 2. åŠ è½½æˆ–æ„å»ºå‘é‡åº“
        if os.path.exists(self.index_path):
            print("æ£€æµ‹åˆ°æœ¬åœ°ç´¢å¼•ï¼Œæ­£åœ¨åŠ è½½...")
            self.vectorstore = FAISS.load_local(
                self.index_path,
                self.embeddings,
                allow_dangerous_deserialization=True
            )
        else:
            print("é¦–æ¬¡æ„å»ºå‘é‡åº“...")
            self.vectorstore = self._build_vectorstore()
            self.vectorstore.save_local(self.index_path)
            print("å‘é‡åº“å·²ä¿å­˜")
        
        # 3. åˆå§‹åŒ–LLM
        self.llm = ChatOpenAI(
            model=Config.RAG_MODEL,
            temperature=0.1,
            max_tokens=2048
        )
        
        # 4. æ„å»ºRAGé“¾
        self._build_rag_chain()
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        total_docs = self.vectorstore.index.ntotal
        print(f"RAGç³»ç»Ÿå°±ç»ªï¼")
        print(f"çŸ¥è¯†åº“åŒ…å«: {total_docs} æ¡æ–‡æ¡£")
        print(f"  â€¢ åœ°åè®°å½•: {self._count_placename_docs()} æ¡")
        print(f"  â€¢ åˆ†ææ´å¯Ÿ: {self._count_insight_docs()} æ¡")
    
    def _build_vectorstore(self):
        """æ„å»ºå‘é‡åº“ - æ•´åˆåœ°åæ•°æ®å’Œåˆ†ææ´å¯Ÿ"""
        documents = []
        
        # 1. åŠ è½½åŸå§‹åœ°åæ•°æ®
        print("åŠ è½½åœ°åæ•°æ®...")
        if os.path.exists(self.data_csv):
            df = pd.read_csv(self.data_csv, encoding='utf-8-sig').fillna("")
            
            # åªåŠ è½½STRONGå’ŒWEAKç±»å‹ï¼ˆæœ‰å‘½åè§£é‡Šçš„ï¼‰
            filtered_df = df[df['resolution_type'].isin(['STRONG', 'WEAK'])]
            
            for _, row in filtered_df.iterrows():
                content = f"åœ°åï¼š{row['placename']}\nè®°è½½ï¼š{row['text']}"
                documents.append(Document(
                    page_content=content,
                    metadata={
                        "type": "placename_record",
                        "source": row['source'],
                        "resolution_type": row['resolution_type'],
                        "placename": row['placename']
                    }
                ))
            
            print(f"  âœ“ å·²åŠ è½½ {len(documents)} æ¡åœ°åè®°å½•")
        
        # 2. åŠ è½½åˆ†ææ´å¯Ÿ
        print("åŠ è½½åˆ†ææ´å¯Ÿ...")
        insights_count = 0
        
        if os.path.exists(self.insights_csv):
            insights_df = pd.read_csv(self.insights_csv, encoding='utf-8-sig').fillna("")
            
            for _, row in insights_df.iterrows():
                # å°†æ¯æ¡æ´å¯Ÿä½œä¸ºä¸€ä¸ªç‹¬ç«‹æ–‡æ¡£
                content = f"ã€{row['category']}ã€‘{row['title']}\n\n{row['content']}"
                documents.append(Document(
                    page_content=content,
                    metadata={
                        "type": "analysis_insight",
                        "category": row['category'],
                        "title": row['title']
                    }
                ))
                insights_count += 1
            
            print(f"  âœ“ å·²åŠ è½½ {insights_count} æ¡åˆ†ææ´å¯Ÿ")
        else:
            print(f"  âš ï¸  æœªæ‰¾åˆ°åˆ†ææ´å¯Ÿæ–‡ä»¶: {self.insights_csv}")
            print(f"     è¯·å…ˆè¿è¡Œ step4_data_analyzer.py ç”Ÿæˆåˆ†æç»“æœ")
        
        # 3. æ·»åŠ æ€»ä½“æ‘˜è¦æ–‡æ¡£ï¼ˆæ–¹ä¾¿å›ç­”å®è§‚é—®é¢˜ï¼‰
        if os.path.exists(self.data_csv):
            df = pd.read_csv(self.data_csv, encoding='utf-8-sig').fillna("")
            
            summary_content = f"""å¤ç±åœ°åæ•°æ®é›†æ€»ä½“æ¦‚å†µï¼š

æœ¬æ•°æ®é›†åŒ…å« {len(df)} æ¡åœ°åè®°å½•ï¼Œåˆ†ç±»å¦‚ä¸‹ï¼š
- STRONGç±»ï¼ˆæ˜ç¡®å‘½åè§£é‡Šï¼‰: {len(df[df['resolution_type']=='STRONG'])} æ¡
- WEAKç±»ï¼ˆå¼•è¯å‘½åè§£é‡Šï¼‰: {len(df[df['resolution_type']=='WEAK'])} æ¡
- NONEç±»ï¼ˆéå‘½åè§£é‡Šï¼‰: {len(df[df['resolution_type']=='NONE'])} æ¡

æ•°æ®æ¥æºæ¶µç›–å¤šéƒ¨å¤ä»£åœ°ç†æ–‡çŒ®ï¼ŒåŒ…æ‹¬å†ä»£æ–¹å¿—ã€åœ°ç†æ€»å¿—ç­‰ã€‚
"""
            documents.append(Document(
                page_content=summary_content,
                metadata={
                    "type": "dataset_summary",
                    "category": "æ€»ä½“æ¦‚å†µ"
                }
            ))
        
        print(f"ğŸ“¦ æ„å»ºå‘é‡åº“: å…± {len(documents)} æ¡æ–‡æ¡£")
        
        return FAISS.from_documents(documents, self.embeddings)
    
    def _build_rag_chain(self):
        """æ„å»ºå¢å¼ºç‰ˆRAGé“¾"""
        # åˆ›å»ºæ£€ç´¢å™¨
        retriever = self.vectorstore.as_retriever(
            search_kwargs={"k": 6}  # å¢åŠ æ£€ç´¢æ•°é‡ï¼ŒåŒæ—¶è·å–åœ°åè®°å½•å’Œæ´å¯Ÿ
        )
        
        # å¢å¼ºç‰ˆæç¤ºè¯æ¨¡æ¿
        template = """ä½ æ˜¯ä¸€åä¸¥è°¨çš„å†å²åœ°ç†å­¦å®¶å’Œæ•°æ®åˆ†æä¸“å®¶ã€‚ä½ å¯ä»¥å›ç­”ä¸¤ç±»é—®é¢˜ï¼š

1. **å…·ä½“åœ°åçš„å‘½åè€ƒæ®é—®é¢˜**ï¼ˆå¦‚"æŸæŸåœ°åçš„ç”±æ¥æ˜¯ä»€ä¹ˆï¼Ÿ"ï¼‰
2. **æ•°æ®ç»Ÿè®¡åˆ†æé—®é¢˜**ï¼ˆå¦‚"æœ‰å¤šå°‘æ¡STRONGè®°å½•ï¼Ÿ""å‘½åé€»è¾‘æœ‰å“ªäº›ç±»å‹ï¼Ÿ"ï¼‰

è¯·æ ¹æ®ä»¥ä¸‹æ£€ç´¢åˆ°çš„ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ï¼š

[æ£€ç´¢åˆ°çš„ä¿¡æ¯]:
{context}

[ç”¨æˆ·æé—®]: {question}

**å›ç­”è¦æ±‚**ï¼š
- å¦‚æœæ˜¯å…·ä½“åœ°åé—®é¢˜ï¼Œé‡ç‚¹å¼•ç”¨ã€åœ°åè®°å½•ã€‘ä¸­çš„åŸæ–‡
- å¦‚æœæ˜¯ç»Ÿè®¡åˆ†æé—®é¢˜ï¼Œé‡ç‚¹å‚è€ƒã€åˆ†ææ´å¯Ÿã€‘ä¸­çš„æ•°æ®
- æ˜ç¡®åŒºåˆ†"ä½œè€…ç›´æ¥é™ˆè¿°"(STRONG)å’Œ"å¼•è¯ä»–äººè¯´æ³•"(WEAK)
- å¼•ç”¨æ—¶æ ‡æ³¨æ¥æºæ–‡çŒ®

[å›ç­”]:"""
        
        prompt = ChatPromptTemplate.from_template(template)
        
        # æ„å»ºLCELé“¾
        self.rag_chain = (
            {"context": retriever, "question": RunnablePassthrough()}
            | prompt
            | self.llm
            | StrOutputParser()
        )
    
    # ==================== æ–°å¢æ–¹æ³• ====================
    
    def _search_with_similarity_scores(self, query: str, k: int = 6):
        """
        æ£€ç´¢å¹¶è¿”å›ç›¸ä¼¼åº¦åˆ†æ•°
        
        Returns:
            [(Document, similarity_score), ...]
            similarity_score: 0-1ä¹‹é—´ï¼Œè¶Šå¤§è¶Šç›¸ä¼¼
        """
        # FAISSè¿”å›çš„æ˜¯è·ç¦»ï¼ˆè¶Šå°è¶Šç›¸ä¼¼ï¼‰ï¼Œè½¬æ¢ä¸ºç›¸ä¼¼åº¦
        docs_and_distances = self.vectorstore.similarity_search_with_score(query, k=k)
        
        # è½¬æ¢ï¼šsimilarity = 1 / (1 + distance)
        docs_with_similarity = []
        for doc, distance in docs_and_distances:
            similarity = 1.0 / (1.0 + distance)
            docs_with_similarity.append((doc, similarity))
        
        return docs_with_similarity
    
    def _rewrite_question(self, original_question: str) -> str:
        """
        æ”¹å†™é—®é¢˜ï¼ˆè®©LLMä¼˜åŒ–è¡¨è¿°ï¼‰
        
        ä¾‹å­ï¼š
        - "è¿™åœ°å„¿å«å•¥ï¼Ÿ" â†’ "è¿™ä¸ªåœ°åçš„ç”±æ¥æ˜¯ä»€ä¹ˆï¼Ÿ"
        """
        rewrite_prompt = f"""ä½ æ˜¯é—®é¢˜ä¼˜åŒ–åŠ©æ‰‹ã€‚ç”¨æˆ·çš„é—®é¢˜å¯èƒ½è¡¨è¾¾ä¸å¤Ÿå‡†ç¡®ï¼Œè¯·æ”¹å†™æˆæ›´é€‚åˆæ£€ç´¢å¤ç±åœ°åæ•°æ®çš„å½¢å¼ã€‚

åŸå§‹é—®é¢˜ï¼š{original_question}

æ”¹å†™è¦æ±‚ï¼š
1. ä¿æŒé—®é¢˜æ ¸å¿ƒæ„å›¾
2. ä½¿ç”¨è§„èŒƒè¡¨è¾¾
3. å¦‚æœæ¶‰åŠåœ°åï¼Œæ˜ç¡®è¯´æ˜"åœ°åçš„ç”±æ¥"æˆ–"å‘½ååŸå› "
4. åªè¿”å›æ”¹å†™åçš„é—®é¢˜ï¼Œä¸è¦è§£é‡Š

æ”¹å†™åï¼š"""
        
        response = self.llm.invoke(rewrite_prompt)
        rewritten = response.content if hasattr(response, 'content') else str(response)
        
        print(f"  ğŸ”„ é—®é¢˜æ”¹å†™: {original_question} â†’ {rewritten}")
        return rewritten.strip()
    
    # ==================== æ”¹è¿›çš„queryæ–¹æ³• ====================
    
    def query(self, user_query: str):
        """
        æ”¹è¿›ç‰ˆæŸ¥è¯¢ - æ–°å¢ç›¸ä¼¼åº¦é˜ˆå€¼æ£€æŸ¥
        
        æµç¨‹ï¼š
        1. æ£€æŸ¥é—®é¢˜ç±»å‹ï¼ˆç»Ÿè®¡ vs å…·ä½“åœ°åï¼‰
        2. æ‰§è¡Œæ£€ç´¢
        3. [æ–°å¢] æ£€æŸ¥ç›¸ä¼¼åº¦
        4. [æ–°å¢] ç›¸ä¼¼åº¦ä½ â†’ æ”¹å†™é—®é¢˜é‡è¯•
        5. [æ–°å¢] ä»ç„¶ä½ â†’ è¿”å›"æ£€ç´¢ä¸åˆ°"
        6. ç›¸ä¼¼åº¦OK â†’ ç”Ÿæˆç­”æ¡ˆ
        """
        q_type = self.get_question_type(user_query)
        
        # ç»Ÿè®¡ç±»é—®é¢˜ï¼šç›´æ¥ç”¨åˆ†ææ´å¯Ÿ
        if q_type == "statistical":
            insights_df = pd.read_csv(self.insights_csv)
            context = "ä»¥ä¸‹æ˜¯å…¨é‡æ•°æ®çš„ç»Ÿè®¡æ´å¯ŸæŠ¥å‘Šï¼š\n" + insights_df.to_string()
            
            full_prompt = f"æ ¹æ®ä»¥ä¸‹ç»Ÿè®¡ä¿¡æ¯å›ç­”é—®é¢˜ï¼š\n\n{context}\n\né—®é¢˜ï¼š{user_query}"
            response = self.llm.invoke(full_prompt)
            if hasattr(response, 'content'):
                return response.content
            return str(response)
        
        # å…·ä½“åœ°åé—®é¢˜ï¼šéœ€è¦ç›¸ä¼¼åº¦æ£€æŸ¥
        print(f"\nğŸ” æŸ¥è¯¢: {user_query}")
        
        # ç¬¬1æ¬¡æ£€ç´¢
        docs_with_sim = self._search_with_similarity_scores(user_query, k=Config.RAG_RETRIEVAL_K)
        max_similarity = max([sim for _, sim in docs_with_sim])
        
        print(f"  ğŸ“Š æœ€é«˜ç›¸ä¼¼åº¦: {max_similarity:.3f}")
        
        # æ£€æŸ¥ç›¸ä¼¼åº¦
        if max_similarity < self.SIMILARITY_THRESHOLD:
            print(f"  âš ï¸  ç›¸ä¼¼åº¦ä½äºé˜ˆå€¼ {self.SIMILARITY_THRESHOLD}")
            
            # å°è¯•æ”¹å†™é—®é¢˜ï¼ˆåªè¯•1æ¬¡ï¼Œé˜²æ­¢å¥—å¨ƒï¼‰
            print(f"  ğŸ”„ å°è¯•æ”¹å†™é—®é¢˜...")
            rewritten_question = self._rewrite_question(user_query)
            
            # ç¬¬2æ¬¡æ£€ç´¢
            docs_with_sim = self._search_with_similarity_scores(rewritten_question, k=Config.RAG_RETRIEVAL_K)
            max_similarity = max([sim for _, sim in docs_with_sim])
            
            print(f"  ğŸ“Š æ”¹å†™åç›¸ä¼¼åº¦: {max_similarity:.3f}")
            
            # ä»ç„¶å¤ªä½ â†’ æ”¾å¼ƒ
            if max_similarity < self.SIMILARITY_THRESHOLD:
                print(f"  âŒ æ”¹å†™åä»ä½äºé˜ˆå€¼")
                
                return f"""æŠ±æ­‰ï¼Œæœªèƒ½æ£€ç´¢åˆ°ä¸'{user_query}'ç›¸å…³çš„å†…å®¹ã€‚

ğŸ’¡ å¯èƒ½çš„åŸå› ï¼š
1. æ‚¨çš„é—®é¢˜å¯èƒ½ä¸åœ¨å¤ç±åœ°åæ•°æ®èŒƒå›´å†…
2. å¯ä»¥å°è¯•æ›´æ¢è¡¨è¿°æ–¹å¼
3. ç¡®è®¤åœ°åæ˜¯å¦åœ¨æ•°æ®åº“ä¸­

ğŸ“š æœ¬ç³»ç»Ÿæ”¯æŒçš„æŸ¥è¯¢ç±»å‹ï¼š
- å…·ä½“åœ°åçš„ç”±æ¥ï¼ˆå¦‚"éš‹å¿çš„ç”±æ¥æ˜¯ä»€ä¹ˆï¼Ÿ"ï¼‰
- ç»Ÿè®¡ç±»é—®é¢˜ï¼ˆå¦‚"æœ‰å¤šå°‘æ¡STRONGè®°å½•ï¼Ÿ"ï¼‰"""
            else:
                print(f"  âœ… æ”¹å†™åç›¸ä¼¼åº¦å¯æ¥å—")
                question_to_use = rewritten_question
        else:
            print(f"  âœ… ç›¸ä¼¼åº¦å¯æ¥å—")
            question_to_use = user_query
        
        # ç›¸ä¼¼åº¦OKï¼Œç”Ÿæˆç­”æ¡ˆ
        docs = [doc for doc, _ in docs_with_sim]
        context = "\n\n".join([doc.page_content for doc in docs])
        
        full_prompt = f"""æ ¹æ®ä»¥ä¸‹æ£€ç´¢åˆ°çš„å¤ç±åœ°åä¿¡æ¯å›ç­”é—®é¢˜ï¼š

{context}

é—®é¢˜ï¼š{question_to_use}

å›ç­”è¦æ±‚ï¼š
- åŸºäºæ£€ç´¢åˆ°çš„ä¿¡æ¯å›ç­”
- å¼•ç”¨æ¥æºæ–‡çŒ®
- å¦‚æœä¿¡æ¯ä¸å®Œæ•´ï¼Œè¯´æ˜å±€é™æ€§"""

        response = self.llm.invoke(full_prompt)
        if hasattr(response, 'content'):
            return response.content
        return str(response)
    
    # ==================== ä¿ç•™åŸæœ‰æ–¹æ³• ====================
    
    def search_documents(self, query: str, k: int = 6):
        """
        æ£€ç´¢ç›¸å…³æ–‡æ¡£ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            k: è¿”å›æ–‡æ¡£æ•°é‡
            
        Returns:
            æ–‡æ¡£åˆ—è¡¨
        """
        docs = self.vectorstore.similarity_search(query, k=k)
        
        result = []
        for i, doc in enumerate(docs, 1):
            result.append({
                "rank": i,
                "type": doc.metadata.get("type", "unknown"),
                "content": doc.page_content[:200] + "...",
                "metadata": doc.metadata
            })
        
        return result
    
    def _count_placename_docs(self) -> int:
        """ç»Ÿè®¡åœ°åè®°å½•æ–‡æ¡£æ•°é‡"""
        try:
            df = pd.read_csv(self.data_csv, encoding='utf-8-sig')
            return len(df[df['resolution_type'].isin(['STRONG', 'WEAK'])])
        except:
            return 0
    
    def _count_insight_docs(self) -> int:
        """ç»Ÿè®¡åˆ†ææ´å¯Ÿæ–‡æ¡£æ•°é‡"""
        try:
            df = pd.read_csv(self.insights_csv, encoding='utf-8-sig')
            return len(df)
        except:
            return 0
    
    def get_question_type(self, question: str) -> str:
        """
        åˆ¤æ–­é—®é¢˜ç±»å‹
        
        Returns:
            'statistical': ç»Ÿè®¡ç±»é—®é¢˜
            'specific': å…·ä½“åœ°åé—®é¢˜
        """
        statistical_keywords = [
            "å¤šå°‘", "æ•°é‡", "æ¯”ä¾‹", "åˆ†å¸ƒ", "ç»Ÿè®¡", "æ€»å…±", "å æ¯”",
            "ç±»å‹", "åˆ†ç±»", "æœ‰å“ªäº›", "ä¸»è¦", "å…¸å‹"
        ]
        
        for keyword in statistical_keywords:
            if keyword in question:
                return "statistical"
        
        return "specific"


def run_interactive_session():
    """è¿è¡Œäº¤äº’å¼é—®ç­”ä¼šè¯"""
    # åˆå§‹åŒ–ç³»ç»Ÿ
    rag = EnhancedRAGSystem()
    rag.setup()
    
    print("\n" + "="*60)
    print("ğŸ›ï¸  å¤ç±åœ°åè€ƒæ®ç³»ç»Ÿï¼ˆå¢å¼ºç‰ˆï¼‰")
    print("="*60)
    print("\nğŸ’¡ æç¤º:")
    print("  â€¢ å¯ä»¥è¯¢é—®å…·ä½“åœ°åçš„å‘½åç”±æ¥")
    print("  â€¢ ä¹Ÿå¯ä»¥è¯¢é—®ç»Ÿè®¡ä¿¡æ¯ï¼ˆå¦‚'æœ‰å¤šå°‘æ¡STRONGè®°å½•ï¼Ÿ'ï¼‰")
    print("  â€¢ [æ–°åŠŸèƒ½] ç›¸ä¼¼åº¦é˜ˆå€¼æ£€æŸ¥ï¼Œé˜²æ­¢èƒ¡ç¼–ä¹±é€ ")
    print("  â€¢ [æ–°åŠŸèƒ½] è‡ªåŠ¨æ”¹å†™é—®é¢˜é‡è¯•")
    print("  â€¢ è¾“å…¥ 'exit' æˆ– 'quit' é€€å‡º")
    print("  â€¢ è¾“å…¥ 'test' æŸ¥çœ‹ç¤ºä¾‹é—®é¢˜")
    print("\n" + "="*60)
    
    while True:
        user_input = input("\nğŸ’¬ è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ > ").strip()
        
        # é€€å‡ºå‘½ä»¤
        if user_input.lower() in ['exit', 'quit']:
            print("\nğŸ‘‹ ç³»ç»Ÿå·²é€€å‡º")
            break
        
        # å¿½ç•¥ç©ºè¾“å…¥
        if not user_input:
            continue
        
        # æµ‹è¯•å‘½ä»¤
        if user_input.lower() == 'test':
            print("\n  ç¤ºä¾‹é—®é¢˜:")
            print("  1. äº¬å¸ˆè¿™ä¸ªåœ°åçš„ç”±æ¥æ˜¯ä»€ä¹ˆï¼Ÿ")
            print("  2. æ•°æ®é›†ä¸­æœ‰å¤šå°‘æ¡STRONGç±»è®°å½•ï¼Ÿ")
            print("  3. å‘½åé€»è¾‘ä¸»è¦æœ‰å“ªäº›ç±»å‹ï¼Ÿ")
            print("  4. ç§¦å§‹çš‡çš„ç”Ÿæ—¥æ˜¯å‡ å·ï¼Ÿ  â† æµ‹è¯•ç›¸ä¼¼åº¦æ£€æŸ¥")
            continue
        
        # æ˜¾ç¤ºé—®é¢˜ç±»å‹
        q_type = rag.get_question_type(user_input)
        if q_type == "statistical":
            print("  [ç»Ÿè®¡åˆ†æé—®é¢˜]")
        else:
            print("  [å…·ä½“åœ°åé—®é¢˜]")
        
        try:
            # æ‰§è¡ŒæŸ¥è¯¢
            answer = rag.query(user_input)
            
            print("\n" + "-"*60)
            print("ğŸ“– å›ç­”:")
            print("-"*60)
            print(answer)
            print("-"*60)
            
        except Exception as e:
            print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")


def main():
    """ä¸»å‡½æ•°"""
    # æ£€æŸ¥å¿…è¦æ–‡ä»¶
    required_files = {
        Config.BATCH_CLASSIFICATION: "åœ°åæ•°æ®æ–‡ä»¶",
        "results/analysis_insights.csv": "åˆ†ææ´å¯Ÿæ–‡ä»¶"
    }
    
    missing_files = []
    for file_path, description in required_files.items():
        if not os.path.exists(file_path):
            missing_files.append((file_path, description))
    
    if missing_files:
        print("âš ï¸  ç¼ºå°‘å¿…è¦æ–‡ä»¶:")
        for file_path, description in missing_files:
            print(f"  â€¢ {file_path} ({description})")
        
        if "results/analysis_insights.csv" in [f[0] for f in missing_files]:
            print("\n  æç¤º: è¯·å…ˆè¿è¡Œ step4_data_analyzer.py ç”Ÿæˆåˆ†æç»“æœ")
        
        return
    
    # è¿è¡Œäº¤äº’å¼ä¼šè¯
    run_interactive_session()


if __name__ == "__main__":
    main()